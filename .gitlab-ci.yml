---

stages:
  - test
  - benchmark
  - execute
  - upload


# job_juwels_compute:
#   stage: execute
#   variables:
#     SCHEDULER_PARAMETERS: '--account=cstma --nodes=1 --partition=devel'
#   tags:
#     - juwels
#     - jacamar
#     - compute
#     - slurm
#   artifacts:
#     paths:
#       - test.file
#   script:
#     - echo $SYSTEMNAME
#     - touch test.file
#   after_script:
#     - hostname
#     - id


variables:
  JUWELS_ACCOUNT: "cstma"


test_JUWELS:
  stage: benchmark
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  tags:
    - jacamar
    - juwels
    - login
    - shell
  parallel:
    matrix:
      - SHELL_SCRIPT: ['benchmark', 'cupy']
  artifacts:
    when: always
    paths:
      - coverage_*.dat
      - sbatch.err
      - sbatch.out
  before_script:
    - mkdir -p benchmarks
    # load the latest Python module (currently 3.11)
    - module --force purge
    - module load Stages/2024
    - module load GCC
    - module load OpenMPI
    - module load FFTW
    - module load mpi4py
    - module load SciPy-Stack
    - module load CuPy
    - pip install -e .
    - pip install pytest-benchmark coverage
  script:
    # - touch benchmarks/output.json
    - echo $SYSTEMNAME
    - sbatch --wait etc/juwels_${SHELL_SCRIPT}.sh
    - touch .coverage.empty
    - python -m coverage combine
    - mv .coverage coverage_${SHELL_SCRIPT}.dat
  after_script:
    - echo "Following Errors occured:"
    - cat sbatch.err
    - echo "Following was written to stdout:"
    - cat sbatch.out


# test_kit:
#  image: rcaspart/micromamba-cuda
#  stage: benchmark
#  variables:
#    USE_NAME: "pySDC-test"
#    SLURM_PARTITION: "dev_accelerated"
#    SLURM_TIME: "00:11:00"
#    SLURM_GRES: "gpu:1"
#  rules:
#    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
#  tags:
#    - kit
#  parallel:
#    matrix:
#      - PYTHON: [ '3.9', '3.10' ]
#  artifacts:
#    name: "gpu_$PYTHON"
#    paths:
#      - coverage_cupy_3.10.dat
#      - data_3.10
#  before_script:
#    - cat /etc/environment
#    - micromamba create --yes python=$PYTHON -f etc/environment-cupy.yml
#    - eval "$(micromamba shell hook --shell=bash)"
#    - micromamba activate pySDC
#    - micromamba install --yes -c conda-forge openssh
#  script:
#    - coverage run --data-file=coverage_cupy_${PYTHON}.dat -m pytest --continue-on-collection-errors -v --durations=0 pySDC/tests -m cupy
#    - mv data data_${PYTHON}


# test_kit_bare:
#  stage: test
#  rules:
#    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-GPU\].*/
#  tags:
#    - kit
#  parallel:
#    matrix:
#      - PYTHON: [ '3.7', '3.9', '3.10' ]
#  artifacts:
#    name: "gpu_$PYTHON"
#    paths:
#      - coverage_cupy_3.10.dat
#      - data_3.10
#  before_script:
#    - module load devel/cuda
#    - curl micro.mamba.pm/install.sh | bash
#    - micromamba create --yes python=$PYTHON -f etc/environment-cupy.yml
#    - micromamba activate pySDC
#  script:
#    - srun -p dev_accelerated -t 00:20:00 -N1 --gres gpu:1 coverage run -m pytest --continue-on-collection-errors -v --durations=0 pySDC/tests -m cupy
#    - coverage combine
#    - mv .coverage coverage_${{ matrix.env }}_${{ matrix.python }}.dat
#    - mv data data_${PYTHON}
#    - chmod +rwx data_${PYTHON}
#    - cat coverage_cupy_${PYTHON}.dat

benchmark:
  image: mambaorg/micromamba
  stage: benchmark
  when: manual
  tags:
    - docker
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  artifacts:
    paths:
      - benchmarks
  before_script:
    - micromamba create --yes -f etc/environment-base.yml
    - eval "$(micromamba shell hook --shell=bash)"
    - micromamba activate pySDC
    - micromamba install -c conda-forge git
    - git config --global --add safe.directory '*'
  script:
    - mkdir -p benchmarks
    - pytest --continue-on-collection-errors -v pySDC/tests -m "benchmark" --benchmark-json=benchmarks/output.json

# benchmark_kit:
#  image: rcaspart/micromamba
#  stage: benchmark
#  variables:
#    USE_NAME: "pySDC-benchmark"
#    SLURM_PARTITION: "dev_cpuonly"
#    SLURM_TIME: "00:11:00"
#  rules:
#    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
#  tags:
#    - kit
#  artifacts:
#    paths:
#      - benchmarks
#  before_script:
#    - micromamba create --yes -f etc/environment-base.yml
#    - eval "$(micromamba shell hook --shell=bash)"
#    - micromamba activate pySDC
#    - micromamba install -c conda-forge git openssh
#    - git config --global --add safe.directory '*'
#  script:
#    - mkdir -p benchmarks
#    - pytest --continue-on-collection-errors -v pySDC/tests -m "benchmark" --benchmark-json=benchmarks/output.json


# bundle:
#  image: mambaorg/micromamba
#  stage: upload
#  artifacts:
#    paths:
#      - data
#      - coverage.xml
#      - benchmarks
#      - htmlcov
#  before_script:
#    - micromamba create --yes -f etc/environment-base.yml
#    - eval "$(micromamba shell hook --shell=bash)"
#    - micromamba activate pySDC
#  script:
#    - cp data_3.10/* data/.
#    - python -m coverage combine coverage_*_3.10.dat
#    - python -m coverage xml
#    - python -m coverage html
