---

stages:
  - test
  - benchmark
  - upload


variables:
  JUWELS_ACCOUNT: "cstma"
  JUWELS_PROJECT: "ccstma"


prepare_JUWELS:
  stage: benchmark
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  id_tokens:
    CI_JOB_JWT:
      aud: https://gitlab.jsc.fz-juelich.de
    SITE_ID_TOKEN:
      aud: https://gitlab.jsc.fz-juelich.de
  tags:
    - jacamar
    - juwels
    - login
    - shell
  script:
    - mkdir -p benchmarks
    - module --force purge
    - module load Stages/2024
    - module load GCC
    - module load OpenMPI
    - module load FFTW
    - module load mpi4py
    - module load SciPy-Stack
    - module load CuPy
    - jutil env activate -p ${JUWELS_PROJECT}
    - python -m venv --clear $SCRATCH/.venv/pySDC
    - source $SCRATCH/.venv/pySDC/bin/activate
    - pip install -e .
    - pip install pytest-benchmark coverage
    - git submodule add -f https://github.com/brownbaerchen/mpi4py-fft.git
    - cd mpi4py-fft
    - git checkout cupy_implementation
    - FFTW_LIBRARY_DIR="/p/software/juwels/stages/2024/software/FFTW/3.3.10-GCC-12.3.0/lib64" pip install --force-reinstall .
    - cd ../


test_JUWELS:
  stage: benchmark
  needs:
    - prepare_JUWELS
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  id_tokens:
    CI_JOB_JWT:
      aud: https://gitlab.jsc.fz-juelich.de
    SITE_ID_TOKEN:
      aud: https://gitlab.jsc.fz-juelich.de
  tags:
    - jacamar
    - juwels
    - login
    - shell
  parallel:
    matrix:
      - SHELL_SCRIPT: ['benchmark', 'cupy']
  artifacts:
    when: always
    paths:
      - coverage_*.dat
      - sbatch.err
      - sbatch.out
  before_script:
    - mkdir -p benchmarks
    - module --force purge
    - module load Stages/2024
    - module load GCC
    - module load OpenMPI
    - module load FFTW
    - module load mpi4py
    - module load SciPy-Stack
    - module load CuPy
    - jutil env activate -p ${JUWELS_PROJECT}
    - source $SCRATCH/.venv/pySDC/bin/activate
  script:
    # - touch benchmarks/output.json
    - echo $SYSTEMNAME
    - sbatch --wait etc/juwels_${SHELL_SCRIPT}.sh
    - touch .coverage.empty
    - python -m coverage combine
    - mv .coverage coverage_${SHELL_SCRIPT}.dat
  after_script:
    - echo "Following Errors occured:"
    - cat sbatch.err
    - echo "Following was written to stdout:"
    - cat sbatch.out


benchmark:
  image: mambaorg/micromamba
  stage: benchmark
  when: manual
  tags:
    - docker
  id_tokens:
    CI_JOB_JWT:
      aud: https://gitlab.jsc.fz-juelich.de
    SITE_ID_TOKEN:
      aud: https://gitlab.jsc.fz-juelich.de
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  artifacts:
    paths:
      - benchmarks
  before_script:
    - micromamba create --yes -f etc/environment-base.yml
    - eval "$(micromamba shell hook --shell=bash)"
    - micromamba activate pySDC
    - micromamba install -c conda-forge git
    - git config --global --add safe.directory '*'
  script:
    - mkdir -p benchmarks
    - >-
      pytest --continue-on-collection-errors -v pySDC/tests -m "benchmark"
      --benchmark-json=benchmarks/output.json

# bundle:
#  image: mambaorg/micromamba
#  stage: upload
#  artifacts:
#    paths:
#      - data
#      - coverage.xml
#      - benchmarks
#      - htmlcov
#  before_script:
#    - micromamba create --yes -f etc/environment-base.yml
#    - eval "$(micromamba shell hook --shell=bash)"
#    - micromamba activate pySDC
#  script:
#    - cp data_3.10/* data/.
#    - python -m coverage combine coverage_*_3.10.dat
#    - python -m coverage xml
#    - python -m coverage html
