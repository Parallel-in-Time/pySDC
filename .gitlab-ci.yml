---

stages:
  - test
  - benchmark
  - upload


variables:
  JUWELS_ACCOUNT: "cstma"
  JUWELS_PROJECT: "ccstma"


prepare_JUWELS:
  stage: benchmark
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  id_tokens:
    CI_JOB_JWT:
      aud: https://gitlab.jsc.fz-juelich.de
    SITE_ID_TOKEN:
      aud: https://gitlab.jsc.fz-juelich.de
  tags:
    - jacamar
    - juwels
    - login
    - shell
  script:
    - mkdir -p benchmarks
    - jutil env activate -p ${JUWELS_PROJECT}
    - source ./etc/venv_juwels/setup.sh


test_JUWELS:
  stage: benchmark
  needs:
    - prepare_JUWELS
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  id_tokens:
    CI_JOB_JWT:
      aud: https://gitlab.jsc.fz-juelich.de
    SITE_ID_TOKEN:
      aud: https://gitlab.jsc.fz-juelich.de
  tags:
    - jacamar
    - juwels
    - login
    - shell
  allow_failure:
    exit_codes:
      - 100
  parallel:
    matrix:
      - SHELL_SCRIPT: ['benchmark', 'cupy']
  artifacts:
    when: always
    paths:
      - coverage_*.dat
      - sbatch.err
      - sbatch.out
  before_script:
    - mkdir -p benchmarks
    - jutil env activate -p ${JUWELS_PROJECT}
    - source ./etc/venv_juwels/activate.sh
  script:
    - bash etc/check_node_avail.sh || PARTITION_AVAIL=$?
    - if [ -n "$PARTITION_AVAIL" ] ; then exit $PARTITION_AVAIL ; fi
    # - touch benchmarks/output.json
    - echo $SYSTEMNAME
    - sbatch --wait etc/juwels_${SHELL_SCRIPT}.sh
    - touch .coverage.empty
    - python -m coverage combine
    - mv .coverage coverage_${SHELL_SCRIPT}.dat
    - echo "Following Errors occured:"
    - cat sbatch.err
    - echo "Following was written to stdout:"
    - cat sbatch.out


benchmark:
  image: mambaorg/micromamba
  stage: benchmark
  when: manual
  tags:
    - docker
  id_tokens:
    CI_JOB_JWT:
      aud: https://gitlab.jsc.fz-juelich.de
    SITE_ID_TOKEN:
      aud: https://gitlab.jsc.fz-juelich.de
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  artifacts:
    paths:
      - benchmarks
  before_script:
    - micromamba create --yes -f etc/environment-base.yml
    - eval "$(micromamba shell hook --shell=bash)"
    - micromamba activate pySDC
    - micromamba install -c conda-forge git
    - git config --global --add safe.directory '*'
  script:
    - mkdir -p benchmarks
    - >-
      pytest --continue-on-collection-errors -v pySDC/tests -m "benchmark"
      --benchmark-json=benchmarks/output.json
