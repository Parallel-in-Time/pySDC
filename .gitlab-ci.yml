---

stages:
  - test
  - benchmark
  - upload


variables:
  JUWELS_ACCOUNT: "cstma"


prepare_JUWELS:
  stage: benchmark
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  tags:
    - jacamar
    - juwels
    - login
    - shell
  script:
    - mkdir -p benchmarks
    - module --force purge
    - module load Stages/2024
    - module load GCC
    - module load OpenMPI
    - module load FFTW
    - module load mpi4py
    - module load SciPy-Stack
    - module load CuPy
    - pip install -e .
    - pip install pytest-benchmark coverage


test_JUWELS:
  stage: benchmark
  needs:
    - prepare_JUWELS
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  tags:
    - jacamar
    - juwels
    - login
    - shell
  parallel:
    matrix:
      - SHELL_SCRIPT: ['benchmark', 'cupy']
  artifacts:
    when: always
    paths:
      - coverage_*.dat
      - sbatch.err
      - sbatch.out
  before_script:
    - mkdir -p benchmarks
    - module --force purge
    - module load Stages/2024
    - module load GCC
    - module load OpenMPI
    - module load FFTW
    - module load mpi4py
    - module load SciPy-Stack
    - module load CuPy
    - git submodule add -f https://github.com/brownbaerchen/mpi4py-fft.git
    - cd mpi4py-fft
    - git checkout cupy_implementation
    - FFTW_LIBRARY_DIR="/p/software/juwels/stages/2024/software/FFTW/3.3.10-GCC-12.3.0/lib64" pip install --force-reinstall -e .
    - cd ../
  script:
    # - touch benchmarks/output.json
    - echo $SYSTEMNAME
    - sbatch --wait etc/juwels_${SHELL_SCRIPT}.sh
    - touch .coverage.empty
    - python -m coverage combine
    - mv .coverage coverage_${SHELL_SCRIPT}.dat
  after_script:
    - echo "Following Errors occured:"
    - cat sbatch.err
    - echo "Following was written to stdout:"
    - cat sbatch.out


#test_kit:
#  image: rcaspart/micromamba-cuda
#  stage: benchmark
#  variables:
#    USE_NAME: "pySDC-test"
#    SLURM_PARTITION: "dev_accelerated"
#    SLURM_TIME: "00:11:00"
#    SLURM_GRES: "gpu:1"
#  rules:
#    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
#  tags:
#    - kit
#  parallel:
#    matrix:
#      - PYTHON: [ '3.9', '3.10' ]
#  artifacts:
#    name: "gpu_$PYTHON"
#    paths:
#      - coverage_cupy_3.10.dat
#      - data_3.10
#  before_script:
#    - cat /etc/environment
#    - micromamba create --yes python=$PYTHON -f etc/environment-cupy.yml
#    - eval "$(micromamba shell hook --shell=bash)"
#    - micromamba activate pySDC
#    - micromamba install --yes -c conda-forge openssh
#  script:
#    - coverage run --data-file=coverage_cupy_${PYTHON}.dat -m pytest --continue-on-collection-errors -v --durations=0 pySDC/tests -m cupy
#    - mv data data_${PYTHON}


#test_kit_bare:
#  stage: test
#  rules:
#    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-GPU\].*/
#  tags:
#    - kit
#  parallel:
#    matrix:
#      - PYTHON: [ '3.7', '3.9', '3.10' ]
#  artifacts:
#    name: "gpu_$PYTHON"
#    paths:
#      - coverage_cupy_3.10.dat
#      - data_3.10
#  before_script:
#    - module load devel/cuda
#    - curl micro.mamba.pm/install.sh | bash
#    - micromamba create --yes python=$PYTHON -f etc/environment-cupy.yml
#    - micromamba activate pySDC
#  script:
#    - srun -p dev_accelerated -t 00:20:00 -N1 --gres gpu:1 coverage run -m pytest --continue-on-collection-errors -v --durations=0 pySDC/tests -m cupy
#    - coverage combine
#    - mv .coverage coverage_${{ matrix.env }}_${{ matrix.python }}.dat
#    - mv data data_${PYTHON}
#    - chmod +rwx data_${PYTHON}
#    - cat coverage_cupy_${PYTHON}.dat

benchmark:
  image: mambaorg/micromamba
  stage: benchmark
  when: manual
  tags:
    - docker
  rules:
    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
  artifacts:
    paths:
      - benchmarks
  before_script:
    - micromamba create --yes -f etc/environment-base.yml
    - eval "$(micromamba shell hook --shell=bash)"
    - micromamba activate pySDC
    - micromamba install -c conda-forge git
    - git config --global --add safe.directory '*'
  script:
    - mkdir -p benchmarks
    - pytest --continue-on-collection-errors -v pySDC/tests -m "benchmark" --benchmark-json=benchmarks/output.json

#benchmark_kit:
#  image: rcaspart/micromamba
#  stage: benchmark
#  variables:
#    USE_NAME: "pySDC-benchmark"
#    SLURM_PARTITION: "dev_cpuonly"
#    SLURM_TIME: "00:11:00"
#  rules:
#    - if: $CI_COMMIT_MESSAGE !~ /.*\[CI-no-benchmarks\]/
#  tags:
#    - kit
#  artifacts:
#    paths:
#      - benchmarks
#  before_script:
#    - micromamba create --yes -f etc/environment-base.yml
#    - eval "$(micromamba shell hook --shell=bash)"
#    - micromamba activate pySDC
#    - micromamba install -c conda-forge git openssh
#    - git config --global --add safe.directory '*'
#  script:
#    - mkdir -p benchmarks
#    - pytest --continue-on-collection-errors -v pySDC/tests -m "benchmark" --benchmark-json=benchmarks/output.json


#bundle:
#  image: mambaorg/micromamba
#  stage: upload
#  artifacts:
#    paths:
#      - data
#      - coverage.xml
#      - benchmarks
#      - htmlcov
#  before_script:
#    - micromamba create --yes -f etc/environment-base.yml
#    - eval "$(micromamba shell hook --shell=bash)"
#    - micromamba activate pySDC
#  script:
#    - cp data_3.10/* data/.
#    - python -m coverage combine coverage_*_3.10.dat
#    - python -m coverage xml
#    - python -m coverage html
