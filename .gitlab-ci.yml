stages:
  - test
  - benchmark
  - upload

#lint:
#  image: mambaorg/micromamba
#  stage: lint
#  before_script:
#    - micromamba create --yes -f etc/environment-lint.yml
#    - eval "$(micromamba shell hook --shell=bash)"
#    - micromamba activate pySDC
#  script:
#    - black pySDC  --check --diff --color
#    - flakeheaven lint --benchmark pySDC

#test-base:
#  image: mambaorg/micromamba
#  stage: test
#  variables:
#    NAME: 'base'
#  parallel:
#    matrix:
#      - PYTHON: [ '3.7', '3.8', '3.9', '3.10' ]
#  artifacts:
#    name: "$NAME_$PYTHON"
#    paths:
#      - coverage.dat
#      - data
#  before_script:
#    - micromamba create --yes python=$PYTHON -f etc/environment-$NAME.yml
#    - eval "$(micromamba shell hook --shell=bash)"
#    - micromamba activate pySDC
#  script:
#    - coverage run --data-file=coverage.dat -m pytest --continue-on-collection-errors -v --durations=0 pySDC/tests -m "not fenics and not petsc and not mpi4py and not benchmark"

test:
  image: mambaorg/micromamba
  stage: test
  parallel:
    matrix:
      - PYTHON: [ '3.7', '3.8', '3.9', '3.10' ]
        NAME: ['base', 'fenics', 'petsc', 'mpi4py']
  artifacts:
    name: "$NAME_$PYTHON"
    paths:
      - coverage_$NAME_$PYTHON.dat
      - data
  before_script:
    - micromamba create --yes python=$PYTHON -f etc/environment-$NAME.yml
    - eval "$(micromamba shell hook --shell=bash)"
    - micromamba activate pySDC
    - micromamba install --yes -c conda-forge openssh
  script:
    - coverage run --data-file=coverage.dat -m pytest --continue-on-collection-errors -svv --durations=0 pySDC/tests -m $NAME

benchmark:
  image: mambaorg/micromamba
  stage: benchmark
  artifacts:
    paths:
      - output.json
  before_script:
    - micromamba create --yes -f etc/environment-base.yml
    - eval "$(micromamba shell hook --shell=bash)"
    - micromamba activate pySDC
  script:
    - pytest --continue-on-collection-errors -v pySDC/tests -m "benchmark" --benchmark-json output.json


bundle:
  image: mambaorg/micromamba
  stage: upload
#  variables:
#    COV_TOKEN: ${COV_TOKEN}
  artifacts:
    paths:
      - data
  before_script:
    - micromamba create --yes -f etc/environment-base.yml
    - eval "$(micromamba shell hook --shell=bash)"
    - micromamba activate pySDC
#    - micromamba install -c conda-forge git
#    - git config --global --add safe.directory '*'
  script:
    - python -m coverage combine coverage_*.dat
    - python -m coverage xml -o data/coverage.xml
    - mv output*.jsob data/
    - ls -artl .
    - ls -artl data/
#    - export COVERALLS_REPO_TOKEN=$COV_TOKEN
#    - export COVERALLS_SERVICE_NAME="gitlab-ci"
#    - export CI_BUILD_NUMBER=$CI_COMMIT_SHORT_SHA
#    - export CI_BUILD_URL=$CI_JOB_URL
#    - export CI_BRANCH=$CI_COMMIT_REF_NAME
#    - coveralls

#upload_results:
#  image: ubuntu:latest
#  stage: upload
#  variables:
#    KEY: ${SSH_PUSH_KEY_GITHUB}
#
#  before_script:
#    - apt-get update -y && apt-get install openssh-client git -y
#    - eval $(ssh-agent -s)
#    - echo "$KEY" | tr -d '\r' | ssh-add -
#    - mkdir -p ~/.ssh
#    - chmod 700 ~/.ssh
#    - ssh-keyscan github.com >> ~/.ssh/known_hosts
#    - chmod 644 ~/.ssh/known_hosts
#    - git config --global user.email "sig.pancetta+github@gmail.com"
#    - git config --global user.name "pancetta"
#  script:
#    - git clone git@github.com:Parallel-in-Time/pySDC-benchmarks.git
#    - cd pySDC-benchmarks
#    - mkdir -p latest-benchmark
#    - cd latest-benchmark
#    - cp ../../*.json .
#    - cd ..
#    - git add -A
#    - git commit -m "adding new benchmark results"
#    - git push
