{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf56b0c",
   "metadata": {},
   "source": [
    "Building your own sweeper\n",
    "--------------------------------\n",
    "While pySDC comes with a lot of functionality, its purpose is to be extendable and (in a sense) modular.\n",
    "Users are able to replace nearly all key functionality with their own approaches.\n",
    "One particular aspect of this are custom sweepers.\n",
    "In this example, we will look at sweepers that enable parallelism across the nodes.\n",
    "\n",
    "A sweeper in pySDC takes values $u_m$ at the nodes and updates them using the SDC iteration rule to get $u_{m+1}$, using the particular type of $Q_\\Delta$ (and of course the scheme itself, be it IMEX, implicit, velocity-verlet and so on).\n",
    "The sweeper is also responsible for computing the residual on a time-step and for getting the final value $u_{\\textrm{end}}$ at the right boundary of the interval.\n",
    "It also provides the mechanism to integrate over the nodes using the $Q$-matrix and it can set an initial guess for each node.\n",
    "\n",
    "Let's look briefly into the maths of the project we have set our minds to.\n",
    "Recall that SDC solves fully implicit Runge-Kutta rules that arise from discretizing the integral version of an initial value problem using a quadrature rule:\n",
    "$$u(t_0+\\Delta t) = \\int_{t_0}^{t_0 + \\Delta t} f(u(t), t) dt + u(t_0) \\rightarrow \\vec{u} = \\Delta t QF(\\vec{u}, \\vec{t}) + \\vec{u}_0.$$\n",
    "The matrix $Q$ carries the quadrature weights, the vector $u$ contains the solutions at the quadrature nodes $\\vec{t}$ (which go from 0 to 1), $F$ is some vector valued operator that evaluates the right hand side at the collocation nodes and $\\vec{u}$ carries the initial conditions in every component.\n",
    "\n",
    "$Q$ is typically dense, which makes solving this directly prohibitively expensive.\n",
    "SDC alleviates this by introducing iterations and a preconditioner.\n",
    "The result is\n",
    "$$(I - \\Delta t Q_\\Delta F)(\\vec{u}^{k+1}) = \\vec{u}_0 + \\Delta t(Q-Q_\\Delta)F(\\vec{u}^k),$$\n",
    "with $Q_\\Delta$ the preconditioner and $k$ the iteration number.\n",
    "\n",
    "Remember that we can solve each iteration by forward substitution because we choose a lower triangular preconditioner.\n",
    "Looking equation component wise, we sweep through the collocation nodes by perform the following \"implicit Euler\" steps:\n",
    "$$u^{k+1}_{m+1} - \\Delta t \\tilde{q}_{m+1, m+1}f(u^{k+1}_{m+1}) = u_0 + \\Delta t \\sum^m_{j=1}\\tilde{q}_{m+1, j}f(u^{k+1}_j) + \\Delta t \\sum^M_{j=1}(q_{m+1, j} - \\tilde{q}_{m+1, j})f(u^{k}_j),$$\n",
    "with $m$ the index of the current collocation node and $M$ the number of collocation nodes.\n",
    "The first part of the right hand side after the initial conditions is the forward subsitituion part which depends on the current iteration at previous collocation nodes.\n",
    "Everything else on the right hand side depends only on the previous iteration.\n",
    "\n",
    "This means if we can eliminate that part, we can update the solutions at the nodes in parallel!\n",
    "This would be nice because it would allow us to parallelize the right hand side evaluations and the \"implicit Euler\" steps across the collocation nodes, which make for the bulk of computation time in SDC.\n",
    "\n",
    "But this part comes from the preconditioner, so the natural thing to do is select a different preconditiner.\n",
    "We get rid of the forward subsitution term by using one that is not just lower triangular, but diagonal.\n",
    "Lucky for us, [someone has looked into this a bit already](https://juser.fz-juelich.de/record/849786/files/Paper.pdf), but generating good diagonal preconditioners is still an active area of research.\n",
    "\n",
    "We will now write a sweeper that uses MPI to indeed parallelize SDC with diagonal preconditioners over $M$ processes.\n",
    "For computing the residual or for integrating over the nodes, communication over the nodes is needed. Note that we will not deal with all the details here, no error checking, no ideal memory layout etc. to keep the example short."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cda423",
   "metadata": {},
   "source": [
    "Before we start, however, let's make sure MPI jobs can be run. We connect to our ``ipcluster`` and check if all looks good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d9f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client()\n",
    "view = rc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1e4184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:1] 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] 4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "\n",
    "print(MPI.COMM_WORLD.Get_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ade1a9",
   "metadata": {},
   "source": [
    "We then start with the initialization of our new sweeper, nothing fancy here.\n",
    "Note the cell magic ``%%px``, which we already used above.\n",
    "This will tell the notebook to run the code in the engine provided by the ``ipcluster`` rather than the kernel you have selected for the notebook.\n",
    "Remember: If you want to run in the ``pySDC_tutorial`` virtual environement, you have to start the cluster in a shell that has this virtual environment activated, because selecting the kernel in the notebook will have no effect on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a3d7432",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import jdc  # required to split the class definition into multiple cells...\n",
    "from pySDC.core.Sweeper import sweeper\n",
    "\n",
    "class generic_implicit_MPI(sweeper):\n",
    "\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            params (dict): Parameters for the sweeper\n",
    "        \"\"\"\n",
    "\n",
    "        # call parent's initialization routine\n",
    "        super().__init__(params)\n",
    "        \n",
    "        # set Q_Delta matrix and store MPI rank\n",
    "        self.QI = self.get_Qdelta_implicit(self.coll, qd_type=self.params.QI)\n",
    "        self.rank = self.params.comm.Get_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e883490",
   "metadata": {},
   "source": [
    "The next thing our sweeper should be able to do is to integrate over the nodes using $Q$. As in the serial case this is just evaluating $Qu$, but now each process has only a single entry of $u$ (i.e. process $m$ has $u_m$). That's a standard use case for ``MPI Reduce``, but each process has to participate in $M$ of those reductions to get all availabe data. Note that there's an easier way to do that if you accept to store not 1 but all $M$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84406e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "%%add_to generic_implicit_MPI\n",
    "def integrate(self):\n",
    "    \"\"\"\n",
    "    Compute dtQF(u).\n",
    "    \"\"\"\n",
    "    # get current level and problem\n",
    "    lvl = self.level\n",
    "    prob = lvl.prob\n",
    "    \n",
    "    assert self.params.comm.size == self.coll.num_nodes, \"This sweeper is only implemented for one collocation node per process.\"\n",
    "\n",
    "    me = prob.dtype_u(prob.init, val=0.0)\n",
    "    for m in range(self.coll.num_nodes):\n",
    "        if m == self.rank:\n",
    "            # if it's my turn, add to result the incoming data\n",
    "            self.params.comm.Reduce(lvl.dt * self.coll.Qmat[m + 1, self.rank + 1] * lvl.f[self.rank + 1],\n",
    "                                    me, root=m, op=MPI.SUM)\n",
    "        else:\n",
    "            # if it's not my turn, contribute to global sum\n",
    "            self.params.comm.Reduce(lvl.dt * self.coll.Qmat[m + 1, self.rank + 1] * lvl.f[self.rank + 1],\n",
    "                                    None, root=m, op=MPI.SUM)\n",
    "    return me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b0411",
   "metadata": {},
   "source": [
    "We end up with the $m$th part of the integral over nodes on process $m$. The next step now is to add the actual sweep mechanics. This is actually very simple: build the right-hand side of SDC and apply the preconditioner. Since $Q_\\Delta$ is diagonal and the nodes are distributed, there is no loop involved (which indicates that we're going to get speeeeeeeedup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9655d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "%%add_to generic_implicit_MPI\n",
    "def update_nodes(self):\n",
    "    # get current level and problem\n",
    "    lvl = self.level\n",
    "    prob = lvl.prob\n",
    "\n",
    "     # start building the right-hand side of for the SDC sweep\n",
    "    # get Q F(u^k)\n",
    "    rhs = self.integrate()\n",
    "    # substract Q_Delta F(u^k)\n",
    "    rhs -= lvl.dt * self.QI[self.rank + 1, self.rank + 1] * lvl.f[self.rank + 1]\n",
    "    # add initial value\n",
    "    rhs += lvl.u[0]\n",
    "\n",
    "    # implicit solve with prefactor stemming from the diagonal of Qd\n",
    "    lvl.u[self.rank + 1] = prob.solve_system(rhs, lvl.dt * self.QI[self.rank + 1, self.rank + 1], lvl.u[self.rank + 1],\n",
    "                                             lvl.time + lvl.dt * self.coll.nodes[self.rank])\n",
    "    # update function values\n",
    "    lvl.f[self.rank + 1] = prob.eval_f(lvl.u[self.rank + 1], lvl.time + lvl.dt * self.coll.nodes[self.rank])\n",
    "\n",
    "    # indicate presence of new values at this level (pySDC internal thing, should always do this in your sweeper)\n",
    "    lvl.status.updated = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e99a6e",
   "metadata": {},
   "source": [
    "Now that there's a way how to get from $u^k$ to $u^{k+1}$, we need to do some bookkeeping: compute the residual, compute the end point, define initial guesses. Nothing fancy here, but necessary nonetheless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92fe844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "%%add_to generic_implicit_MPI\n",
    "def compute_end_point(self):\n",
    "    \"\"\"\n",
    "    Compute the solution at the right interval boundary, which may or may not be a collocation node.\n",
    "    \"\"\"\n",
    "    assert self.coll.right_is_node, \"No implementation for computing end point if it is not a collocation node yet!\"\n",
    "    \n",
    "    # get current level\n",
    "    lvl = self.level\n",
    "    lvl.uend[:] = self.params.comm.bcast(lvl.u[self.rank + 1], root=self.params.comm.Get_size() - 1)\n",
    "\n",
    "def compute_residual(self, stage=None):\n",
    "    \"\"\"\n",
    "    Compute the residual.\n",
    "    \n",
    "    Args:\n",
    "        stage (str): Current stage\n",
    "    \"\"\"\n",
    "    # get current level\n",
    "    lvl = self.level\n",
    "\n",
    "    # compute the residual for each node\n",
    "    res = self.integrate()\n",
    "    res += lvl.u[0] - lvl.u[self.rank + 1]\n",
    "    \n",
    "    # use abs function from data type here\n",
    "    res_norm = abs(res)\n",
    "    \n",
    "    # find maximal residual over the nodes\n",
    "    lvl.status.residual = self.params.comm.allreduce(res_norm, op=MPI.MAX)\n",
    "\n",
    "    # indicate that the residual has seen the new values\n",
    "    lvl.status.updated = False\n",
    "\n",
    "def predict(self):\n",
    "    \"\"\"\n",
    "    This is used before we start iterating to provide an initial guess at all nodes.\n",
    "    \"\"\"\n",
    "    # get current level and problem\n",
    "    lvl = self.level\n",
    "    prob = lvl.prob\n",
    "\n",
    "    # evaluate RHS at left point\n",
    "    lvl.f[0] = prob.eval_f(lvl.u[0], lvl.time)\n",
    "    \n",
    "    # we just implement 'spread' here for simplicity\n",
    "    assert self.params.initial_guess == 'SPREAD', f\"Initial guess \\\"{self.params.initial_guess}\\\" not implemented!\"\n",
    "    lvl.u[self.rank + 1] = prob.dtype_u(lvl.u[0])\n",
    "    lvl.f[self.rank + 1] = prob.eval_f(lvl.u[self.rank + 1], lvl.time + lvl.dt * self.coll.nodes[self.rank])\n",
    "\n",
    "    lvl.uend = prob.dtype_u(lvl.u[0])\n",
    "\n",
    "    # indicate that this level is now ready for sweeps\n",
    "    lvl.status.unlocked = True\n",
    "    lvl.status.updated = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9cbbb",
   "metadata": {},
   "source": [
    "Now we have a full new sweeper which allows us to run in parallel over $M$ nodes (since we did not spend much thought on this, note that it HAS to be exactly $M$ processes.. much room for optimization here!). In order to use all this, we have to pass the new sweeper to the controller and include the MPI communicator as parameter for the sweeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d11f4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from pySDC.implementations.problem_classes.AllenCahn_2D_FD import allencahn_fullyimplicit\n",
    "\n",
    "# initialize problem parameters\n",
    "problem_params = dict()\n",
    "problem_params['nu'] = 2\n",
    "problem_params['eps'] = 0.04\n",
    "problem_params['radius'] = 0.25\n",
    "problem_params['nvars'] = [(128, 128)]\n",
    "problem_params['newton_maxiter'] = 100\n",
    "problem_params['newton_tol'] = 1E-08\n",
    "problem_params['lin_tol'] = 1E-09\n",
    "problem_params['lin_maxiter'] = 100\n",
    "\n",
    "# initialize level parameters\n",
    "level_params = dict()\n",
    "level_params['restol'] = 1E-07\n",
    "level_params['dt'] = 1E-03 / 2\n",
    "level_params['nsweeps'] = 1\n",
    "\n",
    "# initialize sweeper parameters\n",
    "sweeper_params = dict()\n",
    "sweeper_params['quad_type'] = 'RADAU-RIGHT'\n",
    "sweeper_params['num_nodes'] = 4\n",
    "sweeper_params['initial_guess'] = 'SPREAD'\n",
    "\n",
    "# initialize step parameters\n",
    "step_params = dict()\n",
    "step_params['maxiter'] = 50\n",
    "\n",
    "# setup parameters \"in time\"\n",
    "t0 = 0\n",
    "Tend = 0.016\n",
    "\n",
    "# initialize controller parameters\n",
    "controller_params = dict()\n",
    "controller_params['logger_level'] = 30\n",
    "\n",
    "# fill description dictionary for easy step instantiation\n",
    "description = dict()\n",
    "description['problem_class'] = allencahn_fullyimplicit\n",
    "description['problem_params'] = problem_params\n",
    "description['level_params'] = level_params\n",
    "description['step_params'] = step_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a550f23",
   "metadata": {},
   "source": [
    "Now we need to put in our new sweeper. Note that we also have to choose a different $Q_\\Delta$. We'll use a pre-defined one from the base sweeper class, pass the MPI communicator to the sweeper and put all this into the description of the problem to finally instantiate our controller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a77b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from pySDC.implementations.controller_classes.controller_nonMPI import controller_nonMPI\n",
    "\n",
    "sweeper_params['QI'] = ['MIN3']\n",
    "sweeper_params['comm'] = MPI.COMM_WORLD\n",
    "\n",
    "description['sweeper_class'] = generic_implicit_MPI\n",
    "description['sweeper_params'] = sweeper_params\n",
    "\n",
    "controller = controller_nonMPI(num_procs=1, controller_params=controller_params, description=description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f65c1",
   "metadata": {},
   "source": [
    "Looks good so far, so we'll set up the initial conditions and run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c114a425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ede65ecb0e48c79ebd37170c2cd645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/4 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "prob = controller.MS[0].levels[0].prob\n",
    "uinit = prob.u_exact(t0)\n",
    "\n",
    "uend_par, stats_par = controller.run(u0=uinit, t0=t0, Tend=Tend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febbb9a7",
   "metadata": {},
   "source": [
    "OK, no message so far (which is good, logger level is set to silent). All participating processes now have the results and stats, so let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ebe6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:2] Time to solution on rank 2: 8.0760 sec.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] Time to solution on rank 1: 8.0726 sec.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] Time to solution on rank 3: 8.0801 sec.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] Time to solution on rank 0: 8.0595 sec.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2f3f432b71478988c730d7a3a54905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/4 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "rank = MPI.COMM_WORLD.Get_rank()\n",
    "\n",
    "from pySDC.helpers.stats_helper import get_sorted\n",
    "\n",
    "timing = get_sorted(stats_par, type='timing_run', sortby='time')\n",
    "print(f'Time to solution on rank {rank}: {timing[0][1]:6.4f} sec.' )\n",
    "\n",
    "if rank == 0:\n",
    "    import matplotlib.pylab as plt\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"u(t0)\")\n",
    "    plt.imshow(uinit,extent=[-0.5,0.5,-0.5,0.5])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"u(Tend)\")\n",
    "    plt.imshow(uend_par,extent=[-0.5,0.5,-0.5,0.5])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6aa3cb",
   "metadata": {},
   "source": [
    "Anyway, this is how you can use a custom sweeper.\n",
    "And in a very similar manner you can replace everything, from controllers (e.g. for adaptivity or fault tolerance) and the transfer module (e.g. if you need mass matrices) to the step and level classes (e.g. if you need more data available).\n",
    "\n",
    "We hope you felt like getting a first version of the concept into pySDC was not too difficult and you feel encouraged to perhaps try implementing one of your projects some time.\n",
    "\n",
    "Of course, pySDC suffers from the same issues as any other code written by someone else.\n",
    "While object oriented programming allows to avoid copy-pasting lots of code, it can be daunting to sift through myriads of files to find out what why certain details work the way they do.\n",
    "Please know that you can always write us an email or contact us on GitHub if you need some clarification!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySDC_tutorial",
   "language": "python",
   "name": "pysdc_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
